{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e719c9ab",
   "metadata": {},
   "source": [
    "## AI-Powered Complaint Resolution & Root Cause Analysis\n",
    "\n",
    "An end-to-end NLP-driven complaint intelligence platform that automatically classifies customer complaints, identifies root causes, prioritizes urgency, and generates automated responses using LSTM and Transformer models, deployed via a Flask web application.\n",
    "\n",
    "Project Objectives :\n",
    "\n",
    "- Automate complaint classification using deep learning and transformers\n",
    "\n",
    "- Detect sentiment and assign complaint priority\n",
    "\n",
    "- Identify recurring issues and root causes\n",
    "\n",
    "- Generate automated, context-aware replies\n",
    "\n",
    "- Provide a clean web interface for real-time analysis\n",
    "\n",
    "- Stores complaints and AI decisions in a database for internal departments\n",
    "\n",
    "The system is designed for banks, financial institutions, customer support teams, and enterprises.\n",
    " \n",
    "#### Dataset :\n",
    "\n",
    "Source: Consumer Financial Protection Bureau (CFPB)\n",
    "\n",
    "Size: ~228,000 complaints\n",
    "\n",
    "Key Columns Used:\n",
    "\n",
    "Consumer complaint narrative\n",
    "\n",
    "Product\n",
    "\n",
    "Issue\n",
    "\n",
    "Sub-issue\n",
    "\n",
    "https://www.kaggle.com/datasets/kennathalexanderroy/ai-powered-complaint-resolution/data\n",
    "\n",
    "https://www.consumerfinance.gov/data-research/consumer-complaints/#get-the-data\n",
    "\n",
    "\n",
    "#### AI & NLP Techniques Used :\n",
    "\n",
    "| Task                                      | Model / Technique                     |\n",
    "| ----------------------------------------- | ------------------------------------- |\n",
    "| Complaint Classification (Traditional DL) | LSTM (TensorFlow/Keras)               |\n",
    "| Complaint Classification (Advanced NLP)   | Transformer (BART Zero-Shot)          |\n",
    "| Text Preprocessing                        | Tokenization, Lemmatization, Cleaning |\n",
    "| Sentiment Analysis                        | DistilBERT                            |\n",
    "| Priority Assignment                       | Rule-based on sentiment               |\n",
    "| Root Cause Analysis                       | Category mapping logic                |\n",
    "| Automated Replies                         | AI-driven response templates          |\n",
    "\n",
    "\n",
    "#### System Architecture :\n",
    "\n",
    "- User Complaint\n",
    "\n",
    "      ↓\n",
    "\n",
    "- Text Preprocessing (Cleaning + Lemmatization)\n",
    "\n",
    "      ↓\n",
    "\n",
    "- LSTM Classifier ───── Transformer Classifier\n",
    "\n",
    "      ↓                     ↓\n",
    "\n",
    "- Sentiment Analysis (DistilBERT)\n",
    "\n",
    "      ↓\n",
    "\n",
    "- Priority Assignment\n",
    "\n",
    "      ↓\n",
    "\n",
    "- Root Cause Mapping\n",
    "\n",
    "      ↓\n",
    "\n",
    "- Automated Reply Generation\n",
    "\n",
    "      ↓\n",
    "\n",
    "- SQLite Database Storage\n",
    "\n",
    "      ↓\n",
    "\n",
    "- Flask Web Interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb11087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff3ae6",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f061929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Statement Regarding Unauthorized Online Bankin...</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am filing a complaint a\\n\\ngainst Cash App (...</td>\n",
       "      <td>Money transfer, virtual currency, or money ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This account has already been deleted from my ...</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I received the following letter, accepted the ...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am filing a complaint against capital one re...</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Statement Regarding Unauthorized Online Bankin...   \n",
       "1  I am filing a complaint a\\n\\ngainst Cash App (...   \n",
       "2  This account has already been deleted from my ...   \n",
       "3  I received the following letter, accepted the ...   \n",
       "4  I am filing a complaint against capital one re...   \n",
       "\n",
       "                                               label  \n",
       "0  Money transfer, virtual currency, or money ser...  \n",
       "1  Money transfer, virtual currency, or money ser...  \n",
       "2                                    Debt collection  \n",
       "3                                           Mortgage  \n",
       "4                        Checking or savings account  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Pc\\OneDrive\\Desktop\\Alex..New..Folder\\Deep_Learning\\NLP_Project\\Data\\complaints.csv\")\n",
    "\n",
    "df = df[['Consumer complaint narrative', 'Product']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.rename(columns={\n",
    "    'Consumer complaint narrative': 'text',\n",
    "    'Product': 'label'\n",
    "}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aad6f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Debt collection                                       100059\n",
       "Money transfer, virtual currency, or money service     63405\n",
       "Checking or savings account                            51119\n",
       "Mortgage                                               14114\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b719d",
   "metadata": {},
   "source": [
    "### NLP Preprocessing (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a31ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da396e78",
   "metadata": {},
   "source": [
    "### Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "837ba222",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d28d3",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b63e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean_text'],\n",
    "    df['label_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d2c14",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db56847",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b74626",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b708156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_model = Sequential([\n",
    "    Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9695f2",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a35db509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m2859/2859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 176ms/step - accuracy: 0.9034 - loss: 0.2880 - val_accuracy: 0.9055 - val_loss: 0.2697\n",
      "Epoch 2/3\n",
      "\u001b[1m2859/2859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 182ms/step - accuracy: 0.9299 - loss: 0.2075 - val_accuracy: 0.9319 - val_loss: 0.1944\n",
      "Epoch 3/3\n",
      "\u001b[1m2859/2859\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 177ms/step - accuracy: 0.9429 - loss: 0.1620 - val_accuracy: 0.9372 - val_loss: 0.1777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d051152c50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=3,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14f483",
   "metadata": {},
   "source": [
    "### SAVE MODELS (IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9ebb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "lstm_model.save(\"models/lstm_model.h5\")\n",
    "\n",
    "with open(\"models/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"models/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f43f0",
   "metadata": {},
   "source": [
    "### TRANSFORMER MODELS (REAL USAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "975efab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot category classification\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Sentiment\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254abc4",
   "metadata": {},
   "source": [
    "### CATEGORY LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7483982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f4bc67",
   "metadata": {},
   "source": [
    "### FINAL ANALYSIS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d41078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_complaint(text):\n",
    "    cleaned = clean_text(text)\n",
    "\n",
    "    # Transformer category\n",
    "    tf_result = zero_shot_classifier(text, CATEGORIES)\n",
    "    transformer_category = tf_result['labels'][0]\n",
    "\n",
    "    # LSTM category\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    lstm_pred = lstm_model.predict(pad)\n",
    "    lstm_category = label_encoder.inverse_transform(\n",
    "        [np.argmax(lstm_pred)]\n",
    "    )[0]\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment = sentiment_pipeline(text)[0]['label']\n",
    "    priority = \"High\" if sentiment == \"NEGATIVE\" else \"Medium\"\n",
    "\n",
    "    # Root cause + Reply\n",
    "    root_cause_map = {\n",
    "        \"Checking or savings account\": \"Unauthorized transaction\",\n",
    "        \"Debt collection\": \"Incorrect debt collection\",\n",
    "        \"Money transfer\": \"Transaction failure\",\n",
    "        \"Mortgage\": \"Loan servicing issue\",\n",
    "        \"Credit reporting\": \"Incorrect credit report\"\n",
    "    }\n",
    "\n",
    "    reply_map = {\n",
    "        \"Checking or savings account\":\n",
    "        \"We apologize for the unauthorized transaction. Our support team is reviewing your account and will resolve this at the earliest.\",\n",
    "\n",
    "        \"Debt collection\":\n",
    "        \"We apologize for the debt collection issue. Our team is investigating this urgently.\",\n",
    "\n",
    "        \"Money transfer\":\n",
    "        \"We regret the inconvenience caused by the failed transfer. Our team is working on it.\",\n",
    "\n",
    "        \"Mortgage\":\n",
    "        \"We understand your concern regarding your mortgage. Our team is addressing this issue.\",\n",
    "\n",
    "        \"Credit reporting\":\n",
    "        \"We apologize for the incorrect credit reporting. Our team will coordinate with the bureau.\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"Transformer Category\": transformer_category,\n",
    "        \"LSTM Category\": lstm_category,\n",
    "        \"Sentiment\": sentiment,\n",
    "        \"Priority\": priority,\n",
    "        \"Root Cause\": root_cause_map.get(transformer_category),\n",
    "        \"Automated Reply\": reply_map.get(transformer_category)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83781d57",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37d07e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 900ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Transformer Category': 'Checking or savings account',\n",
       " 'LSTM Category': 'Checking or savings account',\n",
       " 'Sentiment': 'NEGATIVE',\n",
       " 'Priority': 'High',\n",
       " 'Root Cause': 'Unauthorized transaction',\n",
       " 'Automated Reply': 'We apologize for the unauthorized transaction. Our support team is reviewing your account and will resolve this at the earliest.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_complaint = \"\"\"\n",
    "There was an unauthorized charge deducted from my savings account\n",
    "without my consent. Customer support did not help.\n",
    "\"\"\"\n",
    "\n",
    "analyze_complaint(test_complaint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
